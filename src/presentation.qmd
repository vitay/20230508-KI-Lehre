---
title: "Artificial Intelligence : ChatGPT"

author: Dr. habil. Julien Vitay
institute: "Professur für künstliche Intelligenz - Fakultät für Informatik - TU Chemnitz"
date: today
date-format: "[08.05.2023]"

footer: "Julien Vitay - Artificial Intelligence : ChatGPT - <https://julien-vitay.net/20230508-KI-Lehre>"
---


# Artficial Intelligence


::: {.columns}
::: {.column width=50%}

* Methods from the field of Artificial Intelligence (AI), especially artificial neural networks (**deep learning**), have revolutionized many technological fields in the last decade:

    * Computer Vision (object recognition, detection, segmentation...)
    
    * Natural Language Processing (translation, text generation, understanding...)
    
    * Control (robotics, video games, Go...)

* The latest generation of **generative models** (Dall-E, Midjourney, ChatGPT) is able to generate an impressive variety of data (images, text, music, source code) and has become available to the general public. 

* The availability of large generative models creates many problems of trustworthiness, ethics, copyright infringements, etc, especially in the context of teaching.

::: 
::: {.column width=50%}

![](img/chatgpt-example.png)

:::
:::

# ChatGPT = GPT-3.5 + RL with human feedback

![Source: <https://openai.com/blog/chatgpt>](img/ChatGPT_Diagram.svg)

# Language models: hierarchical trees


::: {.columns}
::: {.column width=50%}

* One approach to **natural language processing** (NLP) is to use grammatical rules and logical operators to parse sentences and hierarchically assign each word a function.

* Pros: 
    * allows to understand language.
    * benefits from human knowledge.
    * used for compilers.

* Cons: 
    * does not really work...

::: 
::: {.column width=50%}

![Source: <https://web.stanford.edu/~jurafsky/slp3/3.pdf>](img/cfg-1.png)


:::
:::


# Language models: n-gram models



::: {.columns}
::: {.column width=50%}

* The second main approach to NLP is to split a corpus into **n-grams**, i.e. sequences of $n$ consecutive words, and compute conditional probabilities:

$$P(x_n \, | \, x_{1}, \ldots, x_{n-1})$$

* What is the probability of the next word, given the last $n-1$ words were these?

$$P(\text{books} \, | \, \text{the}, \text{students}, \text{opened}, \text{their})$$

![Source: <https://web.stanford.edu/class/cs224n/>](img/nextword.png)

::: 
::: {.column width=50%}

* Bigrams are for example easy to learn in a big $N\times N$ matrix to predict the next word.

* For each pair of consecutive words, just count the number of occurences in the corpus and normalize:

![](img/bigram1.png)

![Source: <https://web.stanford.edu/~jurafsky/slp3/3.pdf>](img/bigram2.png)


:::
:::



# Language models: n-gram models



::: {.columns}
::: {.column width=50%}

* The most likely next word(s) can be **sampled** and proposed for autosuggestion:

![](img/nextword-smartphone.png)

::: 
::: {.column width=50%}

* They can also be used in an **autoregressive** manner, by using the predicted word as the next input:

> The students opened their $\rightarrow$ books
>
> students opened their books $\rightarrow$ and
>
> opened their books and $\rightarrow$ started
>
> ...

* Random sentence generated from a Jane Austen trigram:

> “You are uniformly charming!” cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for.


:::
:::


::: footer
Source: <https://web.stanford.edu/~jurafsky/slp3/3.pdf>
:::


# Language models: n-gram models


![Source: <https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf>](img/nextword.png){width=50%}

* Pros:

    * Very easy to build: just collect a huge corpus of text (books, wikipedia, internet) and create the n-gram matrix.

    * Fast to use: only requires sampling.

    * Statistical model: can stochastically generate different texts depending on sampling.

* Cons:

    * Does not understand anything about the grammar / structure of the language, purely statistical.

    * Does not scale with $n$: the association matrix would be mostly empty.

    * If $n$ is small, there is little context to predict the next word. The generated text is meaningless.

* Can't we build a statistical model of language that scales with $n$? Yes, with neural networks!

# Neural language models


::: {.columns}
::: {.column width=50%}

* All we need is a **parameterized model** that takes $n-1$ word representations as an input and outputs the probability of the next word.

* The model should learn from n-grams found in the data (**supervised learning**).

* One-hot encoded vectors representing words can be built knowing the total number of words in the vocabulary (e.g. 25.000).

![](img/onehotvec.png){width=70%}

::: 
::: {.column width=50%}

![Adapted from: <https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf>](img/blackbox.png)

:::
:::


# Supervised learning

![](img/supervised.png/supervised.png.001.png)

# Supervised neural networks


::: {.columns}
::: {.column width=50%}

* Artificial neural networks (aka deep networks) are parameterized models that hierarchically transform vectors into others, through a sequence of matrix-vector multiplications and non-linear activations functions. 

* The backpropagation algorithm is used to iteratively adapt the weight matrices so that the network minimizes a given loss function on the training set.

* The deeper the network, the more complex I/O relationships it can learn.

* Demo on <https://playground.tensorflow.org>.

::: 
::: {.column width=50%}

![](img/mlp.svg)

:::
:::

# Convolutional neural networks (CNN)

::: {.columns}
::: {.column width=50%}

* Different layer types can be used inside a deep network. Convolutions and max-pooling layers are especially efficient when processing images. 

* The pixels of an image are used as input to the network.

* The output is a vector of 1000 probabilities corresponding to different classes.

![Source: @Krizhevsky2012](img/alexnet.png)

::: 
::: {.column width=50%}

* When trained on the ImageNet dataset (14 million images), CNNs achieve super-human performance in object recognition.

![Source: <https://image-net.org>](img/objrecog.png)

:::
:::

## Object detection

{{< video https://youtu.be/1_SiUOYUoOI width="100%" height="85%" >}}

::: footer
YOLO: You Only Look Once [@Redmon2016a]
:::

## Semantic segmentation


{{< video https://youtu.be/OOT3UIXZztE width="100%" height="85%" >}}

::: footer
Mask R-CNN [@He2018]
:::

# Supervised learning

::: {.columns}
::: {.column width=80%}

* Supervised learning is basically a solved problem using deep neural networks.

* Most major problems in computer vision, speech processing / generation have been solved by CNNs.

* All you need is:

    1. A big enough neural network (dozens of layers)
    2. Tons of annotated data (millions of examples)
    3. A lot of computer power (dozens of high-end GPUs) for a very long training time (weeks)
    4. Money 

* The main problem (for big players like Google, Facebook or OpenAI) is **data annotation**, which has to be done by humans.

* The focus has shifted since a few years to **self-supervised learning**.

:::
::: {.column width=20%}

![@Szegedy2014](img/inception.png)

:::
:::

# Self-supervised learning


::: {.columns}
::: {.column width=50%}


* **Self-supervised learning** (SSL) is the ability to learn something useful from raw data without human annotation.

* The most common approach is to remove some part of existing data and learn to predict it. By doing so, one learns good representations that can be reused.

> Predict the future given the past.

![Context encoder - @Pathak2016](img/context-encoder.png)

::: 
::: {.column width=50%}


* Next-word prediction is a good example of SSL: the next word already present in the raw data and can be used as the ground truth to train a neural network.

![Source:: <https://web.stanford.edu/class/cs224n>](img/blackbox-mlp.png){width=70%}

:::
:::

# Transformer

::: {.columns}
::: {.column width=60%}

* Convolutional neural networks (and recurrent neural networks) have now been replaced by the **Transformer** architecture [@Vaswani2017].

* It is a regular neural network trained using supervised learning, but with a new operation called **self-attention**.

* The original Transformer allowed to translate text in an autoregressive manner (word by word).

![Source: <http://jalammar.github.io/illustrated-transformer/>](img/transformer_decoding_2.gif)

::: 
::: {.column width=40%}

![Source: [@Vaswani2017]](img/transformer-architecture.png)

:::
:::

::: footer
See <https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html>
:::

# Self-attention

* The main advantage of the self-attention operation is that it allows to learn context-dependent representations of the words regardless the length of the sentence:

$$
Z = \text{softmax}(\dfrac{Q \times K^T}{\sqrt{d_k}}) \times V 
$$

* Multiple attention heads allow to learn different contexts for each word.

* Key property: it becomes possible to predict the next word in complete paragraphs / chapters: the only limitation is the computational power!


![Source: <https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html>](img/transformer-needforheads.png){width=70%}


::: footer
See <https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html>
:::

# GPT-3 (Generative Pretrained Transformer)

* GPT-3 is a **Large Language Model** (LLM) from OpenAI, trained to predict the next word (actually token) from a huge text corpus.

* 45 TB of text from Wikipedia, Books, Common Crawl (internet), Webtext2 (Reddit).

![Source: <https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d>](img/gpt-training.webp)

# GPT-3 (Generative Pretrained Transformer)

* GPT-3 is a **Large Language Model** (LLM) from OpenAI, trained to predict the next word (actually token) from a huge text corpus.

* 45 TB of text from Wikipedia, Books, Common Crawl (internet), Webtext2 (Reddit).

![Source: <https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d>](img/gpt-training.gif)



# GPT-3 (Generative Pretrained Transformer)

* It is a subpart of the transformer, but scaled up:

    * 175B parameters, 96 layers, 96 attention heads, vectors of size 12288...

* Demo: <https://platform.openai.com/playground>

![Source: https://jalammar.github.io/illustrated-gpt2/](img/transformer-decoder-intro.png)



# GPT-3 (Generative Pretrained Transformer)

* GPT-3 has been trained to predict the next word on a huge database. 

* Due to its stochastic nature, the produced texts are always different: LLMs are **stochastic parrots** [@Bender2021].

* It presumably took OpenAI 34 days to train GPT-3 on 1024 A100 GPUs (10 k$ each, 5 M$ of electricity). It would take 355 years to train GPT-3 on a single V100 GPU.

![Source: <https://huggingface.co/blog/large-language-models>](img/scaling.jpg)


# Supervised fine-tuning

* GPT-3 can only repeat what it has been trained on. It is not suitable for anything else, such as conversations, question answering or software development, as it is not in the training data

* GPT-3 can be **fine-tuned** with supervised learning using **input prompts** and expected outputs.

* For ChatGPT, GPT-3 was fine-tuned using 13,000 input / output samples written by 40 experts (GPT-3.5).

![Source: <https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d>](img/gpt-finetuning.gif)

# Github Copilot

![Source: <https://copilot.github.com/>](img/githubcopliot.gif)

# ChatGPT = GPT-3.5 + RL with human feedback

![Source: <https://openai.com/blog/chatgpt>](img/ChatGPT_Diagram.svg)

# Reinforcement Learning (RL)

* In RL, the correct answer is not provided. A scalar reward simply signals how good the output is.

* The model has to learn how to maximize rewards on the long-term by producing the best action (output) in the given state (input).

![Source: <https://www.davidsilver.uk/teaching/>](img/rl-loop.png)


::: footer
See <https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/>
:::

# Deep Reinforcement Learning (DRL)

* Given the right loss function, a deep neural network can be trained by **trial-and-error** to maximize the rewards. 

* Many algorithms exist: DQN, A3C, DDPG, PPO, PlaNet, Dreamer, MuZero...

* The main problem is the **reward model**: when is the output good or bad?

::: {}
![](img/deeprl.jpg){width=60% fig-align="center"}
:::

::: footer
See <https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/>
:::

## Atari Games


{{< video https://youtu.be/rQIShnTz1kU width="100%" height="85%" >}}

::: footer
DQN [@Mnih2013]
:::

## Robotics

{{< video https://youtu.be/faDKMMwOS2Q width="100%" height="85%" >}}

::: footer
A3C [@Mnih2016]
:::


# AlphaGo

![](img/alphago.png){width=80%}

* AlphaGo [@Silver2016] was able to beat Lee Sedol in 2016, 19 times World champion.

* It relies on human knowledge to **bootstrap** a RL agent (supervised learning).

* The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use **novel** moves which were never played before and surprised its opponent.

* Training took several weeks on 1202 CPUs and 176 GPUs.


# InstructGPT: Reinforcement Learning from Human Feedback (RLHF)



![Source: <https://huggingface.co/blog/rlhf>](img/reward-model.png)

::: footer
InstructGPT:  [@Ouyang2022]
:::

# Human Feedback

* Human annotation was very cumbersome. Not only did they have to rank answers, but also report NSFW content leaked from reddit.

![](img/chatgpt-hil.webp)


# InstructGPT: Reinforcement Learning from Human Feedback (RLHF)

![Source: <https://huggingface.co/blog/rlhf>](img/rlhf.png)

# Summary

# References