<!DOCTYPE html>
<html lang="en"><head>
<script src="presentation_files/libs/clipboard/clipboard.min.js"></script>
<script src="presentation_files/libs/quarto-html/tabby.min.js"></script>
<script src="presentation_files/libs/quarto-html/popper.min.js"></script>
<script src="presentation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="presentation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="presentation_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="presentation_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.335">

  <meta name="author" content="Dr.&nbsp;habil. Julien Vitay">
  <title>Artificial Intelligence : ChatGPT</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Artificial Intelligence : ChatGPT</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;habil. Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für künstliche Intelligenz - Fakultät für Informatik - TU Chemnitz
          </p>
    </div>
</div>

  <p class="date">08.05.2023</p>
</section>
<section id="artficial-intelligence" class="title-slide slide level1 center">
<h1>Artficial Intelligence</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Methods from the field of Artificial Intelligence (AI), especially artificial neural networks (<strong>deep learning</strong>), have revolutionized many technological fields in the last decade:</p>
<ul>
<li><p>Computer Vision (object recognition, detection, segmentation…)</p></li>
<li><p>Natural Language Processing (translation, text generation, understanding…)</p></li>
<li><p>Control (robotics, video games, Go…)</p></li>
</ul></li>
<li><p>The latest generation of <strong>generative models</strong> (Dall-E, Midjourney, ChatGPT) is able to generate an impressive variety of data (images, text, music, source code) and has become available to the general public.</p></li>
<li><p>The availability of large generative models creates many problems of trustworthiness, ethics, copyright infringements, etc, especially in the context of teaching.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/chatgpt-example.png"></p>
</div>
</div>
</section>

<section id="chatgpt-gpt-3.5-rl-with-human-feedback" class="title-slide slide level1 center">
<h1>ChatGPT = GPT-3.5 + RL with human feedback</h1>

<img data-src="img/ChatGPT_Diagram.svg" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://openai.com/blog/chatgpt" class="uri">https://openai.com/blog/chatgpt</a></p></section>

<section id="language-models-hierarchical-trees" class="title-slide slide level1 center">
<h1>Language models: hierarchical trees</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>One approach to <strong>natural language processing</strong> (NLP) is to use grammatical rules and logical operators to parse sentences and hierarchically assign each word a function.</p></li>
<li><p>Pros:</p>
<ul>
<li>allows to understand language.</li>
<li>benefits from human knowledge.</li>
<li>used for compilers.</li>
</ul></li>
<li><p>Cons:</p>
<ul>
<li>does not really work…</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cfg-1.png"></p>
<p></p><figcaption>Source: <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" class="uri">https://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="language-models-n-gram-models" class="title-slide slide level1 center">
<h1>Language models: n-gram models</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The second main approach to NLP is to split a corpus into <strong>n-grams</strong>, i.e.&nbsp;sequences of <span class="math inline">n</span> consecutive words, and compute conditional probabilities:</li>
</ul>
<p><span class="math display">P(x_n \, | \, x_{1}, \ldots, x_{n-1})</span></p>
<ul>
<li>What is the probability of the next word, given the last <span class="math inline">n-1</span> words were these?</li>
</ul>
<p><span class="math display">P(\text{books} \, | \, \text{the}, \text{students}, \text{opened}, \text{their})</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/nextword.png"></p>
<p></p><figcaption>Source: <a href="https://web.stanford.edu/class/cs224n/" class="uri">https://web.stanford.edu/class/cs224n/</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Bigrams are for example easy to learn in a big <span class="math inline">N\times N</span> matrix to predict the next word.</p></li>
<li><p>For each pair of consecutive words, just count the number of occurences in the corpus and normalize:</p></li>
</ul>
<p><img data-src="img/bigram1.png"></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/bigram2.png"></p>
<p></p><figcaption>Source: <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" class="uri">https://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="language-models-n-gram-models-1" class="title-slide slide level1 center">
<h1>Language models: n-gram models</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The most likely next word(s) can be <strong>sampled</strong> and proposed for autosuggestion:</li>
</ul>
<p><img data-src="img/nextword-smartphone.png"></p>
</div><div class="column" style="width:50%;">
<ul>
<li>They can also be used in an <strong>autoregressive</strong> manner, by using the predicted word as the next input:</li>
</ul>
<blockquote>
<p>The students opened their <span class="math inline">\rightarrow</span> books</p>
<p>students opened their books <span class="math inline">\rightarrow</span> and</p>
<p>opened their books and <span class="math inline">\rightarrow</span> started</p>
<p>…</p>
</blockquote>
<ul>
<li>Random sentence generated from a Jane Austen trigram:</li>
</ul>
<blockquote>
<p>“You are uniformly charming!” cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for.</p>
</blockquote>
</div>
</div>
<div class="footer">
<p>Source: <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" class="uri">https://web.stanford.edu/~jurafsky/slp3/3.pdf</a></p>
</div>
</section>

<section id="language-models-n-gram-models-2" class="title-slide slide level1 center">
<h1>Language models: n-gram models</h1>

<img data-src="img/nextword.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf" class="uri">https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf</a></p><ul>
<li><p>Pros:</p>
<ul>
<li><p>Very easy to build: just collect a huge corpus of text (books, wikipedia, internet) and create the n-gram matrix.</p></li>
<li><p>Fast to use: only requires sampling.</p></li>
<li><p>Statistical model: can stochastically generate different texts depending on sampling.</p></li>
</ul></li>
<li><p>Cons:</p>
<ul>
<li><p>Does not understand anything about the grammar / structure of the language, purely statistical.</p></li>
<li><p>Does not scale with <span class="math inline">n</span>: the association matrix would be mostly empty.</p></li>
<li><p>If <span class="math inline">n</span> is small, there is little context to predict the next word. The generated text is meaningless.</p></li>
</ul></li>
<li><p>Can’t we build a statistical model of language that scales with <span class="math inline">n</span>? Yes, with neural networks!</p></li>
</ul>
</section>

<section id="neural-language-models" class="title-slide slide level1 center">
<h1>Neural language models</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>All we need is a <strong>parameterized model</strong> that takes <span class="math inline">n-1</span> word representations as an input and outputs the probability of the next word.</p></li>
<li><p>The model should learn from n-grams found in the data (<strong>supervised learning</strong>).</p></li>
<li><p>One-hot encoded vectors representing words can be built knowing the total number of words in the vocabulary (e.g.&nbsp;25.000).</p></li>
</ul>
<p><img data-src="img/onehotvec.png" style="width:70.0%"></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/blackbox.png"></p>
<p></p><figcaption>Adapted from: <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf" class="uri">https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture05-rnnlm.pdf</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="supervised-learning" class="title-slide slide level1 center">
<h1>Supervised learning</h1>

<img data-src="img/supervised.png/supervised.png.001.png" class="r-stretch"></section>

<section id="supervised-neural-networks" class="title-slide slide level1 center">
<h1>Supervised neural networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Artificial neural networks (aka deep networks) are parameterized models that hierarchically transform vectors into others, through a sequence of matrix-vector multiplications and non-linear activations functions.</p></li>
<li><p>The backpropagation algorithm is used to iteratively adapt the weight matrices so that the network minimizes a given loss function on the training set.</p></li>
<li><p>The deeper the network, the more complex I/O relationships it can learn.</p></li>
<li><p>Demo on <a href="https://playground.tensorflow.org" class="uri">https://playground.tensorflow.org</a>.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/mlp.svg"></p>
</div>
</div>
</section>

<section>
<section id="convolutional-neural-networks-cnn" class="title-slide slide level1 center">
<h1>Convolutional neural networks (CNN)</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Different layer types can be used inside a deep network. Convolutions and max-pooling layers are especially efficient when processing images.</p></li>
<li><p>The pixels of an image are used as input to the network.</p></li>
<li><p>The output is a vector of 1000 probabilities corresponding to different classes.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/alexnet.png"></p>
<p></p><figcaption>Source: <span class="citation" data-cites="Krizhevsky2012">Krizhevsky et al. (<a href="#/references" role="doc-biblioref" onclick="">2012</a>)</span></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>When trained on the ImageNet dataset (14 million images), CNNs achieve super-human performance in object recognition.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/objrecog.png"></p>
<p></p><figcaption>Source: <a href="https://image-net.org" class="uri">https://image-net.org</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="object-detection" class="slide level2">
<h2>Object detection</h2>
<iframe src="https://www.youtube.com/embed/1_SiUOYUoOI" width="100%" height="85%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<div class="footer">
<p>YOLO: You Only Look Once <span class="citation" data-cites="Redmon2016a">(<a href="#/references" role="doc-biblioref" onclick="">Redmon et al., 2016</a>)</span></p>
</div>
</section>
<section id="semantic-segmentation" class="slide level2">
<h2>Semantic segmentation</h2>
<iframe src="https://www.youtube.com/embed/OOT3UIXZztE" width="100%" height="85%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<div class="footer">
<p>Mask R-CNN <span class="citation" data-cites="He2018">(<a href="#/references" role="doc-biblioref" onclick="">He et al., 2018</a>)</span></p>
</div>
</section></section>
<section id="supervised-learning-1" class="title-slide slide level1 center">
<h1>Supervised learning</h1>
<div class="columns">
<div class="column" style="width:80%;">
<ul>
<li><p>Supervised learning is basically a solved problem using deep neural networks.</p></li>
<li><p>Most major problems in computer vision, speech processing / generation have been solved by CNNs.</p></li>
<li><p>All you need is:</p>
<ol type="1">
<li>A big enough neural network (dozens of layers)</li>
<li>Tons of annotated data (millions of examples)</li>
<li>A lot of computer power (dozens of high-end GPUs) for a very long training time (weeks)</li>
<li>Money</li>
</ol></li>
<li><p>The main problem (for big players like Google, Facebook or OpenAI) is <strong>data annotation</strong>, which has to be done by humans.</p></li>
<li><p>The focus has shifted since a few years to <strong>self-supervised learning</strong>.</p></li>
</ul>
</div><div class="column" style="width:20%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/inception.png"></p>
<p></p><figcaption><span class="citation" data-cites="Szegedy2014">Szegedy et al. (<a href="#/references" role="doc-biblioref" onclick="">2014</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="self-supervised-learning" class="title-slide slide level1 center">
<h1>Self-supervised learning</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Self-supervised learning</strong> (SSL) is the ability to learn something useful from raw data without human annotation.</p></li>
<li><p>The most common approach is to remove some part of existing data and learn to predict it. By doing so, one learns good representations that can be reused.</p></li>
</ul>
<blockquote>
<p>Predict the future given the past.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/context-encoder.png"></p>
<p></p><figcaption>Context encoder - <span class="citation" data-cites="Pathak2016">Pathak et al. (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Next-word prediction is a good example of SSL: the next word already present in the raw data and can be used as the ground truth to train a neural network.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/blackbox-mlp.png" style="width:70.0%"></p>
<p></p><figcaption>Source:: <a href="https://web.stanford.edu/class/cs224n" class="uri">https://web.stanford.edu/class/cs224n</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="transformer" class="title-slide slide level1 center">
<h1>Transformer</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>Convolutional neural networks (and recurrent neural networks) have now been replaced by the <strong>Transformer</strong> architecture <span class="citation" data-cites="Vaswani2017">(<a href="#/references" role="doc-biblioref" onclick="">Vaswani et al., 2017</a>)</span>.</p></li>
<li><p>It is a regular neural network trained using supervised learning, but with a new operation called <strong>self-attention</strong>.</p></li>
<li><p>The original Transformer allowed to translate text in an autoregressive manner (word by word).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/transformer_decoding_2.gif"></p>
<p></p><figcaption>Source: <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/transformer-architecture.png"></p>
<p></p><figcaption>Source: <span class="citation" data-cites="Vaswani2017">(<a href="#/references" role="doc-biblioref" onclick="">Vaswani et al., 2017</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>See <a href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html" class="uri">https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html</a></p>
</div>
</section>

<section id="self-attention" class="title-slide slide level1 center">
<h1>Self-attention</h1>
<ul>
<li>The main advantage of the self-attention operation is that it allows to learn context-dependent representations of the words regardless the length of the sentence:</li>
</ul>
<p><span class="math display">
Z = \text{softmax}(\dfrac{Q \times K^T}{\sqrt{d_k}}) \times V
</span></p>
<ul>
<li><p>Multiple attention heads allow to learn different contexts for each word.</p></li>
<li><p>Key property: it becomes possible to predict the next word in complete paragraphs / chapters: the only limitation is the computational power!</p></li>
</ul>

<img data-src="img/transformer-needforheads.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" class="uri">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></p><div class="footer">
<p>See <a href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html" class="uri">https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/notes/notes/7.1-Transformers.html</a></p>
</div>
</section>

<section id="gpt-3-generative-pretrained-transformer" class="title-slide slide level1 center">
<h1>GPT-3 (Generative Pretrained Transformer)</h1>
<ul>
<li><p>GPT-3 is a <strong>Large Language Model</strong> (LLM) from OpenAI, trained to predict the next word (actually token) from a huge text corpus.</p></li>
<li><p>45 TB of text from Wikipedia, Books, Common Crawl (internet), Webtext2 (Reddit).</p></li>
</ul>

<img data-src="img/gpt-training.webp" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d" class="uri">https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d</a></p></section>

<section id="gpt-3-generative-pretrained-transformer-1" class="title-slide slide level1 center">
<h1>GPT-3 (Generative Pretrained Transformer)</h1>
<ul>
<li><p>GPT-3 is a <strong>Large Language Model</strong> (LLM) from OpenAI, trained to predict the next word (actually token) from a huge text corpus.</p></li>
<li><p>45 TB of text from Wikipedia, Books, Common Crawl (internet), Webtext2 (Reddit).</p></li>
</ul>

<img data-src="img/gpt-training.gif" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d" class="uri">https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d</a></p></section>

<section id="gpt-3-generative-pretrained-transformer-2" class="title-slide slide level1 center">
<h1>GPT-3 (Generative Pretrained Transformer)</h1>
<ul>
<li><p>It is a subpart of the transformer, but scaled up:</p>
<ul>
<li>175B parameters, 96 layers, 96 attention heads, vectors of size 12288…</li>
</ul></li>
<li><p>Demo: <a href="https://platform.openai.com/playground" class="uri">https://platform.openai.com/playground</a></p></li>
</ul>

<img data-src="img/transformer-decoder-intro.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://jalammar.github.io/illustrated-gpt2/</p></section>

<section id="gpt-3-generative-pretrained-transformer-3" class="title-slide slide level1 center">
<h1>GPT-3 (Generative Pretrained Transformer)</h1>
<ul>
<li><p>GPT-3 has been trained to predict the next word on a huge database.</p></li>
<li><p>Due to its stochastic nature, the produced texts are always different: LLMs are <strong>stochastic parrots</strong> <span class="citation" data-cites="Bender2021">(<a href="#/references" role="doc-biblioref" onclick="">Bender et al., 2021</a>)</span>.</p></li>
<li><p>It presumably took OpenAI 34 days to train GPT-3 on 1024 A100 GPUs (10 k$ each, 5 M$ of electricity). It would take 355 years to train GPT-3 on a single V100 GPU.</p></li>
</ul>

<img data-src="img/scaling.jpg" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://huggingface.co/blog/large-language-models" class="uri">https://huggingface.co/blog/large-language-models</a></p></section>

<section id="supervised-fine-tuning" class="title-slide slide level1 center">
<h1>Supervised fine-tuning</h1>
<ul>
<li><p>GPT-3 can only repeat what it has been trained on. It is not suitable for anything else, such as conversations, question answering or software development, as it is not in the training data</p></li>
<li><p>GPT-3 can be <strong>fine-tuned</strong> with supervised learning using <strong>input prompts</strong> and expected outputs.</p></li>
<li><p>For ChatGPT, GPT-3 was fine-tuned using 13,000 input / output samples written by 40 experts (GPT-3.5).</p></li>
</ul>

<img data-src="img/gpt-finetuning.gif" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d" class="uri">https://medium.com/nerd-for-tech/gpt3-and-chat-gpt-detailed-architecture-study-deep-nlp-horse-db3af9de8a5d</a></p></section>

<section id="github-copilot" class="title-slide slide level1 center">
<h1>Github Copilot</h1>

<img data-src="img/githubcopliot.gif" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://copilot.github.com/" class="uri">https://copilot.github.com/</a></p></section>

<section id="chatgpt-gpt-3.5-rl-with-human-feedback-1" class="title-slide slide level1 center">
<h1>ChatGPT = GPT-3.5 + RL with human feedback</h1>

<img data-src="img/ChatGPT_Diagram.svg" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://openai.com/blog/chatgpt" class="uri">https://openai.com/blog/chatgpt</a></p></section>

<section id="reinforcement-learning-rl" class="title-slide slide level1 center">
<h1>Reinforcement Learning (RL)</h1>
<ul>
<li><p>In RL, the correct answer is not provided. A scalar reward simply signals how good the output is.</p></li>
<li><p>The model has to learn how to maximize rewards on the long-term by producing the best action (output) in the given state (input).</p></li>
</ul>

<img data-src="img/rl-loop.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></p><div class="footer">
<p>See <a href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/" class="uri">https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/</a></p>
</div>
</section>

<section>
<section id="deep-reinforcement-learning-drl" class="title-slide slide level1 center">
<h1>Deep Reinforcement Learning (DRL)</h1>
<ul>
<li><p>Given the right loss function, a deep neural network can be trained by <strong>trial-and-error</strong> to maximize the rewards.</p></li>
<li><p>Many algorithms exist: DQN, A3C, DDPG, PPO, PlaNet, Dreamer, MuZero…</p></li>
<li><p>The main problem is the <strong>reward model</strong>: when is the output good or bad?</p></li>
</ul>
<div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/deeprl.jpg" style="width:60.0%"></p>
</figure>
</div>
</div>
<div class="footer">
<p>See <a href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/" class="uri">https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/notes/</a></p>
</div>
</section>
<section id="atari-games" class="slide level2">
<h2>Atari Games</h2>
<iframe src="https://www.youtube.com/embed/rQIShnTz1kU" width="100%" height="85%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<div class="footer">
<p>DQN <span class="citation" data-cites="Mnih2013">(<a href="#/references" role="doc-biblioref" onclick="">Mnih et al., 2013</a>)</span></p>
</div>
</section>
<section id="robotics" class="slide level2">
<h2>Robotics</h2>
<iframe src="https://www.youtube.com/embed/faDKMMwOS2Q" width="100%" height="85%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<div class="footer">
<p>A3C <span class="citation" data-cites="Mnih2016">(<a href="#/references" role="doc-biblioref" onclick="">Mnih et al., 2016</a>)</span></p>
</div>
</section></section>
<section id="alphago" class="title-slide slide level1 center">
<h1>AlphaGo</h1>

<img data-src="img/alphago.png" style="width:80.0%" class="r-stretch"><ul>
<li><p>AlphaGo <span class="citation" data-cites="Silver2016">(<a href="#/references" role="doc-biblioref" onclick="">Silver et al., 2016</a>)</span> was able to beat Lee Sedol in 2016, 19 times World champion.</p></li>
<li><p>It relies on human knowledge to <strong>bootstrap</strong> a RL agent (supervised learning).</p></li>
<li><p>The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use <strong>novel</strong> moves which were never played before and surprised its opponent.</p></li>
<li><p>Training took several weeks on 1202 CPUs and 176 GPUs.</p></li>
</ul>
</section>

<section id="instructgpt-reinforcement-learning-from-human-feedback-rlhf" class="title-slide slide level1 center">
<h1>InstructGPT: Reinforcement Learning from Human Feedback (RLHF)</h1>

<img data-src="img/reward-model.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://huggingface.co/blog/rlhf" class="uri">https://huggingface.co/blog/rlhf</a></p><div class="footer">
<p>InstructGPT: <span class="citation" data-cites="Ouyang2022">(<a href="#/references" role="doc-biblioref" onclick="">Ouyang et al., 2022</a>)</span></p>
</div>
</section>

<section id="human-feedback" class="title-slide slide level1 center">
<h1>Human Feedback</h1>
<ul>
<li>Human annotation was very cumbersome. Not only did they have to rank answers, but also report NSFW content leaked from reddit.</li>
</ul>

<img data-src="img/chatgpt-hil.webp" class="r-stretch"></section>

<section id="instructgpt-reinforcement-learning-from-human-feedback-rlhf-1" class="title-slide slide level1 center">
<h1>InstructGPT: Reinforcement Learning from Human Feedback (RLHF)</h1>

<img data-src="img/rlhf.png" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://huggingface.co/blog/rlhf" class="uri">https://huggingface.co/blog/rlhf</a></p></section>

<section id="summary" class="title-slide slide level1 center">
<h1>Summary</h1>

</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>
<div class="footer footer-default">
<p>Julien Vitay - Artificial Intelligence : ChatGPT - <a href="https://julien-vitay.net/20230508-KI-Lehre" class="uri">https://julien-vitay.net/20230508-KI-Lehre</a></p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Bender2021" class="csl-entry" role="doc-biblioentry">
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the <span>Dangers</span> of <span>Stochastic Parrots</span>: <span>Can Language Models Be Too Big</span>? in <em>Proceedings of the 2021 <span>ACM Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em> <span>FAccT</span> ’21. (<span>New York, NY, USA</span>: <span>Association for Computing Machinery</span>), 610–623. doi:<a href="https://doi.org/10.1145/3442188.3445922">10.1145/3442188.3445922</a>.
</div>
<div id="ref-He2018" class="csl-entry" role="doc-biblioentry">
He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). Mask <span>R-CNN</span>. <a href="http://arxiv.org/abs/1703.06870">http://arxiv.org/abs/1703.06870</a>.
</div>
<div id="ref-Krizhevsky2012" class="csl-entry" role="doc-biblioentry">
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). <span>ImageNet Classification</span> with <span>Deep Convolutional Neural Networks</span>. in <em>Advances in <span>Neural Information Processing Systems</span> (<span>NIPS</span>)</em> <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-Mnih2016" class="csl-entry" role="doc-biblioentry">
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous <span>Methods</span> for <span>Deep Reinforcement Learning</span>. in <em>Proc. <span>ICML</span></em> <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.
</div>
<div id="ref-Mnih2013" class="csl-entry" role="doc-biblioentry">
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing <span>Atari</span> with <span>Deep Reinforcement Learning</span>. <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.
</div>
<div id="ref-Ouyang2022" class="csl-entry" role="doc-biblioentry">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., et al. (2022). Training language models to follow instructions with human feedback. doi:<a href="https://doi.org/10.48550/arXiv.2203.02155">10.48550/arXiv.2203.02155</a>.
</div>
<div id="ref-Pathak2016" class="csl-entry" role="doc-biblioentry">
Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A. A. (2016). Context <span>Encoders</span>: <span>Feature Learning</span> by <span>Inpainting</span>. doi:<a href="https://doi.org/10.48550/arXiv.1604.07379">10.48550/arXiv.1604.07379</a>.
</div>
<div id="ref-Redmon2016a" class="csl-entry" role="doc-biblioentry">
Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You <span>Only Look Once</span>: <span>Unified</span>, <span>Real-Time Object Detection</span>. <a href="http://arxiv.org/abs/1506.02640">http://arxiv.org/abs/1506.02640</a>.
</div>
<div id="ref-Silver2016" class="csl-entry" role="doc-biblioentry">
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of <span>Go</span> with deep neural networks and tree search. <em>Nature</em> 529, 484–489. doi:<a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.
</div>
<div id="ref-Szegedy2014" class="csl-entry" role="doc-biblioentry">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2014). Going <span>Deeper</span> with <span>Convolutions</span>. <a href="http://arxiv.org/abs/1409.4842">http://arxiv.org/abs/1409.4842</a>.
</div>
<div id="ref-Vaswani2017" class="csl-entry" role="doc-biblioentry">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention <span>Is All You Need</span>. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="presentation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="presentation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>